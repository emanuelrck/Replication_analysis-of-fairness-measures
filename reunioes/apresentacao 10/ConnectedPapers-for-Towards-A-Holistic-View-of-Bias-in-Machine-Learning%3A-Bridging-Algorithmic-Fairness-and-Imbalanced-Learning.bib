@article{4a1a64c91c58d414bf207bca67aa8d9792680950,
title = {Towards A Holistic View of Bias in Machine Learning: Bridging Algorithmic Fairness and Imbalanced Learning},
year = {2022},
url = {https://www.semanticscholar.org/paper/4a1a64c91c58d414bf207bca67aa8d9792680950},
abstract = {Machine learning (ML) is playing an increasingly important role in rendering decisions that affect a broad range of groups in society. This posits the requirement of algorithmic fairness, which holds that automated decisions should be equitable with respect to protected features (e.g., gender, race). Training datasets can contain both class imbalance and protected feature bias. We postulate that, to be effective, both class and protected feature bias should be reduced—which allows for an increase in model accuracy and fairness. Our method, Fair OverSampling (FOS), uses SMOTE (Chawla in J Artif Intell Res 16:321–357, 2002) to reduce class imbalance and feature blurring to enhance group fairness. Because we view bias in imbalanced learning and algorithmic fairness differently, we do not attempt to balance classes and features; instead, we seek to de-bias features and balance the number of class instances. FOS restores numerical class balance through the creation of synthetic minority class instances and causes a classifier to pay less attention to protected features. Therefore, it reduces bias for both classes and protected features. Additionally, we take a step toward bridging the gap between fairness and imbalanced learning with a new metric, Fair Utility, that measures model effectiveness with respect to accuracy and fairness. Our source code and data are publicly available at https://github.com/dd1github/Fair-Over-Sampling.},
author = {Damien Dablain and B. Krawczyk and N. Chawla},
journal = {ArXiv},
volume = {abs/2207.06084},
pages = {null},
doi = {10.48550/arXiv.2207.06084},
arxivid = {2207.06084},
}

@article{c3b88967e28099dd32dfa936c1696a79ae3c1657,
title = {On the Power of Randomization in Fair Classification and Representation},
year = {2022},
url = {https://www.semanticscholar.org/paper/c3b88967e28099dd32dfa936c1696a79ae3c1657},
abstract = {Fair classification and fair representation learning are two important problems in supervised and unsupervised fair machine learning, respectively. Fair classification asks for a classifier that maximizes accuracy on a given data distribution subject to fairness constraints. Fair representation maps a given data distribution over the original feature space to a distribution over a new representation space such that all classifiers over the representation satisfy fairness. In this paper, we examine the power of randomization in both these problems to minimize the loss of accuracy that results when we impose fairness constraints. Previous work on fair classification has characterized the optimal fair classifiers on a given data distribution that maximize accuracy subject to fairness constraints, e.g., Demographic Parity (DP), Equal Opportunity (EO), and Predictive Equality (PE). We refine these characterizations to demonstrate when the optimal randomized fair classifiers can surpass their deterministic counterparts in accuracy. We also show how the optimal randomized fair classifier that we characterize can be obtained as a solution to a convex optimization problem. Recent work has provided techniques to construct fair representations for a given data distribution such that any classifier over this representation satisfies DP. However, the classifiers on these fair representations either come with no or weak accuracy guarantees when compared to the optimal fair classifier on the original data distribution. Extending our ideas for randomized fair classification, we improve on these works, and construct DP-fair, EO-fair, and PE-fair representations that have provably optimal accuracy and suffer no accuracy loss compared to the optimal DP-fair, EO-fair, and PE-fair classifiers respectively on the original data distribution.},
author = {Sushant Agarwal and A. Deshpande},
journal = {Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
volume = {null},
pages = {null},
doi = {10.1145/3531146.3533209},
}

@article{0048e59bac5b981b71a1b8bb43ca26302ae4abf3,
title = {Learning fair representations via an adversarial framework},
year = {2023},
url = {https://www.semanticscholar.org/paper/0048e59bac5b981b71a1b8bb43ca26302ae4abf3},
abstract = {null},
author = {Huadong Qiu and Rui Feng and Ruoyun Hu and Xiao Yang and Shaowa Lin and Quanjin Tao and Yang Yang},
journal = {AI Open},
volume = {4},
pages = {91-97},
doi = {10.1016/j.aiopen.2023.08.003},
arxivid = {1904.13341},
}

@article{0343fc06fa7b63d38d5ce27bc72355cc8b6a9718,
title = {Towards Threshold Invariant Fair Classification},
year = {2020},
url = {https://www.semanticscholar.org/paper/0343fc06fa7b63d38d5ce27bc72355cc8b6a9718},
abstract = {Effective machine learning models can automatically learn useful information from a large quantity of data and provide decisions in a high accuracy. These models may, however, lead to unfair predictions in certain sense among the population groups of interest, where the grouping is based on such sensitive attributes as race and gender. Various fairness definitions, such as demographic parity and equalized odds, were proposed in prior art to ensure that decisions guided by the machine learning models are equitable. Unfortunately, the "fair" model trained with these fairness definitions is threshold sensitive, i.e., the condition of fairness may no longer hold true when tuning the decision threshold. This paper introduces the notion of threshold invariant fairness, which enforces equitable performances across different groups independent of the decision threshold. To achieve this goal, this paper proposes to equalize the risk distributions among the groups via two approximation methods. Experimental results demonstrate that the proposed methodology is effective to alleviate the threshold sensitivity in machine learning models designed to achieve fairness.},
author = {Mingliang Chen and Min Wu},
journal = {ArXiv},
volume = {abs/2006.10667},
pages = {null},
arxivid = {2006.10667},
}

@article{f657885194683083d3ff10d4956ff6f0b60a405c,
title = {MAAT: a novel ensemble approach to addressing fairness and performance bugs for machine learning software},
year = {2022},
url = {https://www.semanticscholar.org/paper/f657885194683083d3ff10d4956ff6f0b60a405c},
abstract = {Machine Learning (ML) software can lead to unfair and unethical decisions, making software fairness bugs an increasingly significant concern for software engineers. However, addressing fairness bugs often comes at the cost of introducing more ML performance (e.g., accuracy) bugs. In this paper, we propose MAAT, a novel ensemble approach to improving fairness-performance trade-off for ML software. Conventional ensemble methods combine different models with identical learning objectives. MAAT, instead, combines models optimized for different objectives: fairness and ML performance. We conduct an extensive evaluation of MAAT with 5 state-of-the-art methods, 9 software decision tasks, and 15 fairness-performance measurements. The results show that MAAT significantly outperforms the state-of-the-art. In particular, MAAT beats the trade-off baseline constructed by a recent benchmarking tool in 92.2% of the overall cases evaluated, 12.2 percentage points more than the best technique currently available. Moreover, the superiority of MAAT over the state-of-the-art holds on all the tasks and measurements that we study. We have made publicly available the code and data of this work to allow for future replication and extension.},
author = {Zhenpeng Chen and J Zhang and Federica Sarro and M. Harman},
journal = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
volume = {null},
pages = {null},
doi = {10.1145/3540250.3549093},
}

@article{274763dcc74da716a5f282d88b268309ed642f90,
title = {Using Balancing Terms to Avoid Discrimination in Classification},
year = {2018},
url = {https://www.semanticscholar.org/paper/274763dcc74da716a5f282d88b268309ed642f90},
abstract = {From personalized ad delivery and healthcare to criminal sentencing, more decisions are made with help from methods developed in the fields of data mining and machine learning than ever before. However, their widespread use has raised concerns about the discriminatory impact which the methods may have on people subject to these decisions. Recently, imbalance in the misclassification rates between groups has been identified as a source of discrimination. Such discrimination is not handled by most existing work in discrimination-aware data mining, and it can persist even if other types of discrimination are alleviated. In this article, we present the Balancing Terms (BT) method to address this problem. BT balances the error rates of any classifier with a differentiable prediction function, and unlike existing work, it can incorporate a preference for the trade-off between fairness and accuracy. We empirically evaluate BT on real-world data, demonstrating that our method produces tradeoffs between error rate balance and total classification error that are superior and in only few cases comparable to the state-of-the-art.},
author = {Simon Enni and I. Assent},
journal = {2018 IEEE International Conference on Data Mining (ICDM)},
volume = {null},
pages = {947-952},
doi = {10.1109/ICDM.2018.00116},
}

@article{5649071b4199bfd625a8f3a3fca3d455fd81cc65,
title = {A Comprehensive Empirical Study of Bias Mitigation Methods for Software Fairness},
year = {2022},
url = {https://www.semanticscholar.org/paper/5649071b4199bfd625a8f3a3fca3d455fd81cc65},
abstract = {Software bias is an increasingly important operational concern for software engineers. We present a large-scale, comprehensive empirical evaluation of 17 representative bias mitigation methods, evaluated with 12 Machine Learning (ML) performance metrics, 4 fairness metrics, and 24 types of fairness-performance trade-off assessment, applied to 8 widely-adopted benchmark software decision/prediction tasks. The empirical coverage is comprehensive, covering the largest numbers of bias mitigation methods, evaluation metrics, and fairness-performance trade-off measures compared to previous work on this important operational software characteristic. We find that (1) the bias mitigation methods significantly decrease the values reported by all ML performance metrics (including those not considered in previous work) in a large proportion of the scenarios studied (42% ∼ 75% according to different ML performance metrics); (2) the bias mitigation methods achieve fairness improvement in only approximately 50% over all scenarios and metrics (ranging between 29% ∼ 59% according to the metric used to asses bias/fairness); (3) the bias mitigation methods have a poor fairness-performance trade-off or even lead to decreases in both fairness and ML performance in 37% of the scenarios; (4) the effectiveness of the bias mitigation methods depends on tasks, models, and fairness and ML performance metrics, and there is no ‘silver bullet’ bias mitigation method demonstrated to be effective for all scenarios studied. The best bias mitigation method that we find outperforms other methods in only 29% of the scenarios. We have made publicly available the scripts and data used in this study in order to allow for future replication and extension of our work. may not use the latter as their proxy. This finding suggests that we take comprehensive ML performance metrics into account during the evaluation of bias mitigation methods, especially considering that different ML performance metrics measure the functional properties of ML software from different aspects and thus may provide different guidelines for real-world application scenarios. 56%). This finding further demonstrates the necessity of this study that measures the trade-off in terms of various fairness-performance metric pairs to obtain more general and comprehensive results.},
author = {Zhenpeng Chen and J Zhang and Federica Sarro and M. Harman},
journal = {ArXiv},
volume = {abs/2207.03277},
pages = {null},
doi = {10.48550/arXiv.2207.03277},
}

@article{9717a45063565658ea9201686edb2f138e025daf,
title = {Distraction is All You Need for Fairness},
year = {2022},
url = {https://www.semanticscholar.org/paper/9717a45063565658ea9201686edb2f138e025daf},
abstract = {Bias in training datasets must be managed for various groups in classification tasks to ensure parity or equal treatment. With the recent growth in artificial intelligence models and their expanding role in automated decision-making, ensuring that these models are not biased is vital. There is an abundance of evidence suggesting that these models could contain or even amplify the bias present in the data on which they are trained, inherent to their objective function and learning algorithms; Many researchers direct their attention to this issue in different directions, namely, changing data to be statistically independent, adversarial training for restricting the capabilities of a particular competitor who aims to maximize parity, etc. These methods result in information loss and do not provide a suitable balance between accuracy and fairness or do not ensure limiting the biases in training. To this end, we propose a powerful strategy for training deep learning models called the Distraction module, which can be theoretically proven effective in controlling bias from affecting the classification results. This method can be utilized with different data types (e.g., Tabular, images, graphs, etc.). We demonstrate the potency of the proposed method by testing it on UCI Adult and Heritage Health datasets (tabular), POKEC-Z, POKEC-N and NBA datasets (graph), and CelebA dataset (vision). Using state-of-the-art methods proposed in the fairness literature for each dataset, we exhibit our model is superior to these proposed methods in minimizing bias and maintaining accuracy.},
author = {Mehdi Yazdani-Jahromi and Amirarsalan Rajabi and Aida Tayebi and O. Garibay},
journal = {ArXiv},
volume = {abs/2203.07593},
pages = {null},
doi = {10.48550/arXiv.2203.07593},
arxivid = {2203.07593},
}

@article{b399726b5106edd712321759f56b29393a69f2b4,
title = {Bias in machine learning software: why? how? what to do?},
year = {2021},
url = {https://www.semanticscholar.org/paper/b399726b5106edd712321759f56b29393a69f2b4},
abstract = {Increasingly, software is making autonomous decisions in case of criminal sentencing, approving credit cards, hiring employees, and so on. Some of these decisions show bias and adversely affect certain social groups (e.g. those defined by sex, race, age, marital status). Many prior works on bias mitigation take the following form: change the data or learners in multiple ways, then see if any of that improves fairness. Perhaps a better approach is to postulate root causes of bias and then applying some resolution strategy. This paper postulates that the root causes of bias are the prior decisions that affect- (a) what data was selected and (b) the labels assigned to those examples. Our Fair-SMOTE algorithm removes biased labels; and rebalances internal distributions such that based on sensitive attribute, examples are equal in both positive and negative classes. On testing, it was seen that this method was just as effective at reducing bias as prior approaches. Further, models generated via Fair-SMOTE achieve higher performance (measured in terms of recall and F1) than other state-of-the-art fairness improvement algorithms. To the best of our knowledge, measured in terms of number of analyzed learners and datasets, this study is one of the largest studies on bias mitigation yet presented in the literature.},
author = {Joymallya Chakraborty and Suvodeep Majumder and T. Menzies},
journal = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
volume = {null},
pages = {null},
doi = {10.1145/3468264.3468537},
arxivid = {2105.12195},
}

@article{acd6de3ac2a3d9449aae51b87fbb03f6f0020954,
title = {The Measure and Mismeasure of Fairness: A Critical Review of Fair Machine Learning},
year = {2018},
url = {https://www.semanticscholar.org/paper/acd6de3ac2a3d9449aae51b87fbb03f6f0020954},
abstract = {The nascent field of fair machine learning aims to ensure that decisions guided by algorithms are equitable. Over the last several years, three formal definitions of fairness have gained prominence: (1) anti-classification, meaning that protected attributes---like race, gender, and their proxies---are not explicitly used to make decisions; (2) classification parity, meaning that common measures of predictive performance (e.g., false positive and false negative rates) are equal across groups defined by the protected attributes; and (3) calibration, meaning that conditional on risk estimates, outcomes are independent of protected attributes. Here we show that all three of these fairness definitions suffer from significant statistical limitations. Requiring anti-classification or classification parity can, perversely, harm the very groups they were designed to protect; and calibration, though generally desirable, provides little guarantee that decisions are equitable. In contrast to these formal fairness criteria, we argue that it is often preferable to treat similarly risky people similarly, based on the most statistically accurate estimates of risk that one can produce. Such a strategy, while not universally applicable, often aligns well with policy objectives; notably, this strategy will typically violate both anti-classification and classification parity. In practice, it requires significant effort to construct suitable risk estimates. One must carefully define and measure the targets of prediction to avoid retrenching biases in the data. But, importantly, one cannot generally address these difficulties by requiring that algorithms satisfy popular mathematical formalizations of fairness. By highlighting these challenges in the foundation of fair machine learning, we hope to help researchers and practitioners productively advance the area.},
author = {S. Corbett-Davies and Sharad Goel},
journal = {ArXiv},
volume = {abs/1808.00023},
pages = {null},
}

@article{7164fa1319ce3abf2a3a43f448631a9bdcd0c5c9,
title = {FairBalance: How to Achieve Equalized Odds With Data Pre-processing},
year = {2021},
url = {https://www.semanticscholar.org/paper/7164fa1319ce3abf2a3a43f448631a9bdcd0c5c9},
abstract = {This research seeks to benefit the software engineering society by providing a simple yet effective pre-processing approach to achieve equalized odds fairness in machine learning software. Fairness issues have attracted increasing attention since machine learning software is increasingly used for high-stakes and high-risk decisions. Amongst all the existing fairness notions, this work specifically targets"equalized odds"given its advantage in always allowing perfect classifiers. Equalized odds requires that members of every demographic group do not receive disparate mistreatment. Prior works either optimize for an equalized odds related metric during the learning process like a black-box, or manipulate the training data following some intuition. This work studies the root cause of the violation of equalized odds and how to tackle it. We found that equalizing the class distribution in each demographic group with sample weights is a necessary condition for achieving equalized odds without modifying the normal training process. In addition, an important partial condition for equalized odds (zero average odds difference) can be guaranteed when the class distributions are weighted to be not only equal but also balanced (1:1). Based on these analyses, we proposed FairBalance, a pre-processing algorithm which balances the class distribution in each demographic group by assigning calculated weights to the training data. On eight real-world datasets, our empirical results show that, at low computational overhead, the proposed pre-processing algorithm FairBalance can significantly improve equalized odds without much, if any damage to the utility. FairBalance also outperforms existing state-of-the-art approaches in terms of equalized odds. To facilitate reuse, reproduction, and validation, we made our scripts available at https://github.com/hil-se/FairBalance.},
author = {Zhe Yu and Joymallya Chakraborty and T. Menzies},
arxivid = {2107.08310},
}

@article{5be95c628832823853a3da5dae213a7fee05f4d0,
title = {FNNC: Achieving Fairness through Neural Networks},
year = {2018},
url = {https://www.semanticscholar.org/paper/5be95c628832823853a3da5dae213a7fee05f4d0},
abstract = {In classification models, fairness can be ensured by solving a constrained optimization problem. We focus on fairness constraints like Disparate Impact, Demographic Parity, and Equalized Odds, which are non-decomposable and non-convex. Researchers define convex surrogates of the constraints and then apply convex optimization frameworks to obtain fair classifiers. Surrogates serve as an upper bound to the actual constraints, and convexifying fairness constraints is challenging.



We propose a neural network-based framework, \emph{FNNC}, to achieve fairness while maintaining high accuracy in classification. The above fairness constraints are included in the loss using Lagrangian multipliers. We prove bounds on generalization errors for the constrained losses which asymptotically go to zero. The network is optimized using two-step mini-batch stochastic gradient descent. Our experiments show that FNNC performs as good as the state of the art, if not better. The experimental evidence supplements our theoretical guarantees. In summary, we have an automated solution to achieve fairness in classification, which is easily extendable to many fairness constraints.},
author = {P. Manisha and Sujit Gujar},
doi = {10.24963/ijcai.2020/315},
arxivid = {1811.00247},
}

@article{1b811adfb5b75a08216f287b819244e835878aaa,
title = {Privileged and Unprivileged Groups: An Empirical Study on the Impact of the Age Attribute on Fairness},
year = {2022},
url = {https://www.semanticscholar.org/paper/1b811adfb5b75a08216f287b819244e835878aaa},
abstract = {Recent advances in software fairness investigate bias in the treatment of different population groups, which are devised based on attributes such as gender, race and age. Groups are divided into privileged groups (favourable treatment) and unprivileged groups (unfavourable treatment). To truthfully represent the real world and to measure the degree of bias according to age (young vs. old), one needs to pick a threshold to separate those groups. In this study we investigate two popular datasets (i.e., German and Bank) and the bias observed when using every possible age threshold in order to divide the population into “young” and “old” groups, in combination with three different Machine Learning models (i.e., Logistic Regression, Decision Tree, Support Vector Machine). Our results show that age thresholds do not only impact the intensity of bias in these datasets, but also the direction (i.e., which population group receives a favourable outcome). For the two investigated datasets, we present a selection of suitable age thresholds. We also found strong and very strong correlations between the dataset bias and the respective bias of trained classification models, in 83% of the cases studied. CCS CONCEPTS • Social and professional topics → User characteristics; • General and reference → Empirical studies. ACM Reference Format: Max Hort and Federica Sarro. 2022. Privileged and Unprivileged Groups: An Empirical Study on the Impact of the Age Attribute on Fairness. In International Workshop on Equirable Data and Technology (FairWare ’22), May 9, 2022, Pittsburgh, PA, USA. ACM, New York, NY, USA, 8 pages. https://doi.org/10.1145/3524491.3527308},
author = {Max Hort and Federica Sarro},
journal = {2022 IEEE/ACM International Workshop on Equitable Data & Technology (FairWare)},
volume = {null},
pages = {17-24},
doi = {10.1145/3524491.3527308},
}

@article{18537166bc2c67b564cf4ecda53dd126bf5c9c19,
title = {Fair-SSL: Building fair ML Software with less data},
year = {2021},
url = {https://www.semanticscholar.org/paper/18537166bc2c67b564cf4ecda53dd126bf5c9c19},
abstract = {Ethical bias in machine learning models has become a matter of concern in the software engineering community. Most of the prior software engineering works concentrated on finding ethical bias in models rather than fixing it. After finding bias, the next step is mitigation. Prior researchers mainly tried to use supervised approaches to achieve fairness. However, in the real world, getting data with trustworthy ground truth is challenging and also ground truth can contain human bias. Semi-supervised learning is a technique where, incrementally, labeled data is used to generate pseudo-labels for the rest of data (and then all that data is used for model training). In this work, we apply four popular semi-supervised techniques as pseudo-labelers to create fair classification models. Our framework, Fair-SSL, takes a very small amount (10%) of labeled data as input and generates pseudo-labels for the unlabeled data. We then synthetically generate new data points to balance the training data based on class and protected attribute as proposed by Chakraborty et al. in FSE 2021. Finally, classification model is trained on the balanced pseudo-labeled data and validated on test data. After experimenting on ten datasets and three learners, we find that Fair-SSL achieves similar performance as three state-of-the-art bias mitigation algorithms. That said, the clear advantage of Fair-SSL is that it requires only 10% of the labeled training data. To the best of our knowledge, this is the first SE work where semi-supervised techniques are used to fight against ethical bias in SE ML models. To facilitate open science and replication, all our source code and datasets are publicly available at https://github.com/joymallyac/FairSSL. CCS CONCEPTS • Software and its engineering → Software creation and management; • Computing methodologies → Machine learning. ACM Reference Format: Joymallya Chakraborty, Suvodeep Majumder, and Huy Tu. 2022. Fair-SSL: Building fair ML Software with less data. In International Workshop on Equitable Data and Technology (FairWare ‘22), May 9, 2022, Pittsburgh, PA, USA. ACM, New York, NY, USA, 8 pages. https://doi.org/10.1145/3524491.3527305},
author = {Joymallya Chakraborty and Suvodeep Majumder and Huy Tu},
journal = {2022 IEEE/ACM International Workshop on Equitable Data & Technology (FairWare)},
volume = {null},
pages = {1-8},
doi = {10.1145/3524491.3527305},
arxivid = {2111.02038},
}

@article{39b1988007f1a8d519bc1825b2dd9c6c7c6c5971,
title = {Bayes-Optimal Classifiers under Group Fairness},
year = {2022},
url = {https://www.semanticscholar.org/paper/39b1988007f1a8d519bc1825b2dd9c6c7c6c5971},
abstract = {Machine learning algorithms are becoming integrated into more and more high-stakes decision-making processes, such as in social welfare issues. Due to the need of mitigating the potentially disparate impacts from algorithmic predictions, many approaches have been proposed in the emerging area of fair machine learning. However, the fundamental problem of characterizing Bayes-optimal classifiers under various group fairness constraints has only been investigated in some special cases. Based on the classical Neyman-Pearson argument (Neyman and Pearson, 1933; Shao, 2003) for optimal hypothesis testing, this paper provides a unified framework for deriving Bayes-optimal classifiers under group fairness. This enables us to propose a group-based thresholding method we call FairBayes, that can directly control disparity, and achieve an essentially optimal fairness-accuracy tradeoff. These advantages are supported by thorough experiments.},
author = {Xianli Zeng and Edgar Dobriban and Guang Cheng},
journal = {ArXiv},
volume = {abs/2202.09724},
pages = {null},
arxivid = {2202.09724},
}

@article{cd8a2bee8510e05691e8e83532739dca2229d99b,
title = {Developing a novel fair-loan-predictor through a multi-sensitive debiasing pipeline: DualFair},
year = {2021},
url = {https://www.semanticscholar.org/paper/cd8a2bee8510e05691e8e83532739dca2229d99b},
abstract = {Machine learning (ML) models are increasingly used for high-stake applications that can greatly impact people's lives. Despite their use, these models have the potential to be biased towards certain social groups on the basis of race, gender, or ethnicity. Many prior works have attempted to mitigate this"model discrimination"by updating the training data (pre-processing), altering the model learning process (in-processing), or manipulating model output (post-processing). However, these works have not yet been extended to the realm of multi-sensitive parameters and sensitive options (MSPSO), where sensitive parameters are attributes that can be discriminated against (e.g race) and sensitive options are options within sensitive parameters (e.g black or white), thus giving them limited real-world usability. Prior work in fairness has also suffered from an accuracy-fairness tradeoff that prevents both the accuracy and fairness from being high. Moreover, previous literature has failed to provide holistic fairness metrics that work with MSPSO. In this paper, we solve all three of these problems by (a) creating a novel bias mitigation technique called DualFair and (b) developing a new fairness metric (i.e. AWI) that can handle MSPSO. Lastly, we test our novel mitigation method using a comprehensive U.S mortgage lending dataset and show that our classifier, or fair loan predictor, obtains better fairness and accuracy metrics than current state-of-the-art models.},
author = {Ashutosh Kumar Singh and Jashandeep Singh and Ariba Khan and Amar Gupta},
journal = {ArXiv},
volume = {abs/2110.08944},
pages = {null},
arxivid = {2110.08944},
}

@article{1c4671b35e1f93708dfb763d17141902e4a24654,
title = {A Comprehensive Empirical Study of Bias Mitigation Methods for Machine Learning Classifiers},
year = {2022},
url = {https://www.semanticscholar.org/paper/1c4671b35e1f93708dfb763d17141902e4a24654},
abstract = {Software bias is an increasingly important operational concern for software engineers. We present a large-scale, comprehensive empirical study of 17 representative bias mitigation methods for Machine Learning (ML) classifiers, evaluated with 11 ML performance metrics (e.g., accuracy), 4 fairness metrics, and 20 types of fairness-performance tradeoff assessment, applied to 8 widely-adopted software decision tasks. The empirical coverage is much more comprehensive, covering the largest numbers of bias mitigation methods, evaluation metrics, and fairness-performance tradeoff measures compared to previous work on this important software property. We find that (1) the bias mitigation methods significantly decrease ML performance in 53% of the studied scenarios (ranging between 42%∼66% according to different ML performance metrics); (2) the bias mitigation methods significantly improve fairness measured by the 4 used metrics in 46% of all the scenarios (ranging between 24%∼59% according to different fairness metrics); (3) the bias mitigation methods even lead to decrease in both fairness and ML performance in 25% of the scenarios; (4) the effectiveness of the bias mitigation methods depends on tasks, models, the choice of protected attributes, and the set of metrics used to assess fairness and ML performance; (5) there is no bias mitigation method that can achieve the best tradeoff in all the scenarios. The best method that we find outperforms other methods in 30% of the scenarios. Researchers and practitioners need to choose the bias mitigation method best suited to their intended application scenario(s).},
author = {Zhenpeng Chen and J Zhang and Federica Sarro and M. Harman},
journal = {ACM Transactions on Software Engineering and Methodology},
volume = {32},
pages = {1 - 30},
doi = {10.1145/3583561},
arxivid = {2207.03277},
}

@article{a013b1e1acdd78207049049fd20b57e4f51ed01b,
title = {A Maximal Correlation Approach to Imposing Fairness in Machine Learning},
year = {2020},
url = {https://www.semanticscholar.org/paper/a013b1e1acdd78207049049fd20b57e4f51ed01b},
abstract = {As machine learning algorithms grow in popularity and diversify to many industries, ethical and legal concerns regarding their fairness have become increasingly relevant. We explore the problem of algorithmic fairness, taking an information-theoretic view. The maximal correlation framework is introduced for expressing fairness constraints and shown to be capable of deriving regularizers that enforce independence and separation-based fairness criteria, which admit optimization algorithms that are more computationally efficient than existing algorithms. We show that these algorithms provide smooth performance-fairness tradeoff curves and perform competitively with state-of-the-art methods on the Communities and Crimes dataset.},
author = {Joshua K. Lee and Yuheng Bu and P. Sattigeri and Rameswar Panda and G. Wornell and Leonid Karlinsky and R. Feris},
journal = {ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
volume = {null},
pages = {3523-3527},
doi = {10.1109/icassp43922.2022.9747263},
arxivid = {2012.15259},
}

@article{4959054adba3e7160976a1d5179e1ba283522820,
title = {Training Data Debugging for the Fairness of Machine Learning Software},
year = {2022},
url = {https://www.semanticscholar.org/paper/4959054adba3e7160976a1d5179e1ba283522820},
abstract = {With the widespread application of machine learning (ML) software, especially in high-risk tasks, the concern about their unfairness has been raised towards both developers and users of ML software. The unfairness of ML software indicates the software behavior affected by the sensitive features (e.g., sex), which leads to biased and illegal decisions and has become a worthy problem for the whole software engineering community. According to the “data-driven” programming paradigm of ML software, we consider the root cause of the unfairness as biased features in training data. Inspired by software debugging, we propose a novel method, Linear-regression based Training Data Debugging (LTDD), to debug feature values in training data, i.e., (a) identify which features and which parts of them are biased, and (b) exclude the biased parts of such features to recover as much valuable and unbiased information as possible to build fair ML software. We conduct an extensive study on nine data sets and three classifiers to evaluate the effect of our method LTDD compared with four baseline methods. Experimental results show that (a) LTDD can better improve the fairness of ML software with less or comparable damage to the performance, and (b) LTDD is more actionable for fairness improvement in realistic scenarios.},
author = {Yanhui Li and Linghan Meng and Lin Chen and Li Yu and Di Wu and Yuming Zhou and Baowen Xu},
journal = {2022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)},
volume = {null},
pages = {2215-2227},
doi = {10.1145/3510003.3510091},
}

@article{58335384abc874f7c8935163d0fcef4fcaaf7bbb,
title = {An Empirical Study on Fairness Improvement with Multiple Protected Attributes},
year = {2023},
url = {https://www.semanticscholar.org/paper/58335384abc874f7c8935163d0fcef4fcaaf7bbb},
abstract = {Existing research mostly improves the fairness of Machine Learning (ML) software regarding a single protected attribute at a time, but this is unrealistic given that many users have multiple protected attributes. This paper conducts an extensive study of fairness improvement regarding multiple protected attributes, covering 11 state-of-the-art fairness improvement methods. We analyze the effectiveness of these methods with different datasets, metrics, and ML models when considering multiple protected attributes. The results reveal that improving fairness for a single protected attribute can largely decrease fairness regarding unconsidered protected attributes. This decrease is observed in up to 88.3% of scenarios (57.5% on average). More surprisingly, we find little difference in accuracy loss when considering single and multiple protected attributes, indicating that accuracy can be maintained in the multiple-attribute paradigm. However, the effect on precision and recall when handling multiple protected attributes is about 5 times and 8 times that of a single attribute. This has important implications for future fairness research: reporting only accuracy as the ML performance metric, which is currently common in the literature, is inadequate.},
author = {Zhenpeng Chen and Jie M. Zhang and Federica Sarro and M. Harman},
journal = {ArXiv},
volume = {abs/2308.01923},
pages = {null},
doi = {10.48550/arXiv.2308.01923},
arxivid = {2308.01923},
}

@article{00c6b956b754f54898101dd19330cdadc9e0c483,
title = {Metrics and methods for a systematic comparison of fairness-aware machine learning algorithms},
year = {2020},
url = {https://www.semanticscholar.org/paper/00c6b956b754f54898101dd19330cdadc9e0c483},
abstract = {Understanding and removing bias from the decisions made by machine learning models is essential to avoid discrimination against unprivileged groups. Despite recent progress in algorithmic fairness, there is still no clear answer as to which bias-mitigation approaches are most effective. Evaluation strategies are typically use-case specific, rely on data with unclear bias, and employ a fixed policy to convert model outputs to decision outcomes. To address these problems, we performed a systematic comparison of a number of popular fairness algorithms applicable to supervised classification. Our study is the most comprehensive of its kind. It utilizes three real and four synthetic datasets, and two different ways of converting model outputs to decisions. It considers fairness, predictive-performance, calibration quality, and speed of 28 different modelling pipelines, corresponding to both fairness-unaware and fairness-aware algorithms. We found that fairness-unaware algorithms typically fail to produce adequately fair models and that the simplest algorithms are not necessarily the fairest ones. We also found that fairness-aware algorithms can induce fairness without material drops in predictive power. Finally, we found that dataset idiosyncracies (e.g., degree of intrinsic unfairness, nature of correlations) do affect the performance of fairness-aware approaches. Our results allow the practitioner to narrow down the approach(es) they would like to adopt without having to know in advance their fairness requirements.},
author = {Gareth Jones and James M. Hickey and Pietro G. Di Stefano and C. Dhanjal and Laura C. Stoddart and V. Vasileiou},
journal = {ArXiv},
volume = {abs/2010.03986},
pages = {null},
arxivid = {2010.03986},
}

@article{0a08cdd184ee52b579aa82a70aee0168fa67ea88,
title = {Optimized Score Transformation for Fair Classification},
year = {2019},
url = {https://www.semanticscholar.org/paper/0a08cdd184ee52b579aa82a70aee0168fa67ea88},
abstract = {This paper considers fair probabilistic classification where the outputs of primary interest are predicted probabilities, commonly referred to as scores. We formulate the problem of transforming scores to satisfy fairness constraints that are linear in conditional means of scores while minimizing the loss in utility. The formulation can be applied either to post-process classifier outputs or to pre-process training data, thus allowing maximum freedom in selecting a classification algorithm. We derive a closed-form expression for the optimal transformed scores and a convex optimization problem for the transformation parameters. In the population limit, the transformed score function is the fairness-constrained minimizer of cross-entropy with respect to the optimal unconstrained scores. In the finite sample setting, we propose to approach this solution using a combination of standard probabilistic classifiers and ADMM. The transformation parameters obtained from the finite-sample procedure are shown to be asymptotically optimal. Comprehensive experiments comparing to 10 existing methods show that the proposed FairScoreTransformer has advantages for score-based metrics such as Brier score and AUC while remaining competitive for binary label-based metrics such as accuracy.},
author = {Dennis Wei and K. Ramamurthy and F. Calmon},
journal = {ArXiv},
volume = {abs/1906.00066},
pages = {null},
}

@article{dcd136593fd786a23d8265297f114dff00c280ae,
title = {Quantifying Infra-Marginality and Its Trade-off with Group Fairness},
year = {2019},
url = {https://www.semanticscholar.org/paper/dcd136593fd786a23d8265297f114dff00c280ae},
abstract = {In critical decision-making scenarios, optimizing accuracy can lead to a biased classifier, hence past work recommends enforcing group-based fairness metrics in addition to maximizing accuracy. However, doing so exposes the classifier to another kind of bias called infra-marginality. This refers to individual-level bias where some individuals/subgroups can be worse off than under simply optimizing for accuracy. For instance, a classifier implementing race-based parity may significantly disadvantage women of the advantaged race. To quantify this bias, we propose a general notion of $\eta$-infra-marginality that can be used to evaluate the extent of this bias. We prove theoretically that, unlike other fairness metrics, infra-marginality does not have a trade-off with accuracy: high accuracy directly leads to low infra-marginality. This observation is confirmed through empirical analysis on multiple simulated and real-world datasets. Further, we find that maximizing group fairness often increases infra-marginality, suggesting the consideration of both group-level fairness and individual-level infra-marginality. However, measuring infra-marginality requires knowledge of the true distribution of individual-level outcomes correctly and explicitly. We propose a practical method to measure infra-marginality, and a simple algorithm to maximize group-wise accuracy and avoid infra-marginality.},
author = {Arpita Biswas and Siddharth Barman and A. Deshpande and Amit Sharma},
journal = {ArXiv},
volume = {abs/1909.00982},
pages = {null},
arxivid = {1909.00982},
}

@article{6945e0e7bda8701dd3083672d4d38a701379f5cf,
title = {Benchmarking Bias Mitigation Algorithms in Representation Learning through Fairness Metrics},
year = {2021},
url = {https://www.semanticscholar.org/paper/6945e0e7bda8701dd3083672d4d38a701379f5cf},
abstract = {With the recent expanding attention of machine learning researchers and 1 practitioners to fairness, there is a void of a common framework to analyze and 2 compare the capabilities of proposed models in deep representation learning. In this 3 paper, we evaluate diﬀerent fairness methods trained with deep neural networks on 4 a common synthetic dataset and a real-world dataset to obtain better insights on how 5 these methods work. In particular, we train about 3000 diﬀerent models in various 6 setups, including imbalanced and correlated data conﬁgurations, to verify the limits 7 of the current models and better understand in which setups they are subject to 8 failure. Our results show that the bias of models increase as datasets become more 9 imbalanced or datasets attributes become more correlated, the level of dominance 10 of correlated sensitive dataset features impact bias, and the sensitive information 11 remains in the latent representation even when bias-mitigation algorithms are 12 applied. Overall, we present a dataset, propose various challenging evaluation 13 setups, and rigorously evaluate recent promising bias-mitigation algorithms in 14 a common framework and publicly release this benchmark † , hoping the research 15 community would take it as a common entry point for fair deep learning. 16},
author = {Charan Reddy and Deepak Sharma and Soroush Mehri and Adriana Romero-Soriano and Samira Shabanian and Sina Honari},
}

@article{f4615ae853fa0e8effbc5b36f7455c43520345aa,
title = {On Fairness and Calibration},
year = {2017},
url = {https://www.semanticscholar.org/paper/f4615ae853fa0e8effbc5b36f7455c43520345aa},
abstract = {The machine learning community has become increasingly concerned with the potential for bias and discrimination in predictive models. This has motivated a growing line of work on what it means for a classification procedure to be "fair." In this paper, we investigate the tension between minimizing error disparity across different population groups while maintaining calibrated probability estimates. We show that calibration is compatible only with a single error constraint (i.e. equal false-negatives rates across groups), and show that any algorithm that satisfies this relaxation is no better than randomizing a percentage of predictions for an existing classifier. These unsettling findings, which extend and generalize existing results, are empirically confirmed on several datasets.},
author = {Geoff Pleiss and Manish Raghavan and Felix Wu and J. Kleinberg and Kilian Q. Weinberger},
journal = {The Societal Impacts of Algorithmic Decision-Making},
volume = {null},
pages = {null},
doi = {10.1145/3603195.3603198},
arxivid = {1709.02012},
}

@article{1b0754797f8674e1b6b52c14e561ef36a8903f68,
title = {"Ignorance and Prejudice" in Software Fairness},
year = {2021},
url = {https://www.semanticscholar.org/paper/1b0754797f8674e1b6b52c14e561ef36a8903f68},
abstract = {Machine learning software can be unfair when making human-related decisions, having prejudices over certain groups of people. Existing work primarily focuses on proposing fairness metrics and presenting fairness improvement approaches. It remains unclear how key aspect of any machine learning system, such as feature set and training data, affect fairness. This paper presents results from a comprehensive study that addresses this problem. We find that enlarging the feature set plays a significant role in fairness (with an average effect rate of 38%). Importantly, and contrary to widely-held beliefs that greater fairness often corresponds to lower accuracy, our findings reveal that an enlarged feature set has both higher accuracy and fairness. Perhaps also surprisingly, we find that a larger training data does not help to improve fairness. Our results suggest a larger training data set has more unfairness than a smaller one when feature sets are insufficient; an important cautionary finding for practising software engineers.},
author = {J Zhang and M. Harman},
journal = {2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)},
volume = {null},
pages = {1436-1447},
doi = {10.1109/ICSE43902.2021.00129},
}

@article{0f98f7772ffb5c0de686cbc1014800c4a28aedd7,
title = {Did You Do Your Homework? Raising Awareness on Software Fairness and Discrimination},
year = {2021},
url = {https://www.semanticscholar.org/paper/0f98f7772ffb5c0de686cbc1014800c4a28aedd7},
abstract = {Machine Learning is a vital part of various modern day decision making software. At the same time, it has shown to exhibit bias, which can cause an unjust treatment of individuals and population groups. One method to achieve fairness in machine learning software is to provide individuals with the same degree of benefit, regardless of sensitive attributes (e.g., students receive the same grade, independent of their sex or race). However, there can be other attributes that one might want to discriminate against (e.g., students with homework should receive higher grades). We will call such attributes anti-protected attributes. When reducing the bias of machine learning software, one risks the loss of discriminatory behaviour of anti-protected attributes. To combat this, we use grid search to show that machine learning software can be debiased (e.g., reduce gender bias) while also improving the ability to discriminate against anti-protected attributes.},
author = {Max Hort and Federica Sarro},
journal = {2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)},
volume = {null},
pages = {1322-1326},
doi = {10.1109/ASE51524.2021.9678568},
}

@article{f701776f39296c6ac620cedc3696499529e7c5ba,
title = {Fair Transfer Learning with Factor Variational Auto-Encoder},
year = {2022},
url = {https://www.semanticscholar.org/paper/f701776f39296c6ac620cedc3696499529e7c5ba},
abstract = {S2 TL;DR: A new training model with information-theoretically motivated objective which avoids the problem of alignment for learning disentangled fair representations is brought in for fair transfer learning where the labels of the downstream tasks are unknown.},
author = {Shaofan Liu and Shiliang Sun and Jing Zhao},
journal = {Neural Processing Letters},
volume = {55},
pages = {2049-2061},
doi = {10.1007/s11063-022-10920-8},
}

@article{1f02aa1cb619c8ba989e3e5d72d5c204e8fbe102,
title = {Fair preprocessing: towards understanding compositional fairness of data transformers in machine learning pipeline},
year = {2021},
url = {https://www.semanticscholar.org/paper/1f02aa1cb619c8ba989e3e5d72d5c204e8fbe102},
abstract = {In recent years, many incidents have been reported where machine learning models exhibited discrimination among people based on race, sex, age, etc. Research has been conducted to measure and mitigate unfairness in machine learning models. For a machine learning task, it is a common practice to build a pipeline that includes an ordered set of data preprocessing stages followed by a classifier. However, most of the research on fairness has considered a single classifier based prediction task. What are the fairness impacts of the preprocessing stages in machine learning pipeline? Furthermore, studies showed that often the root cause of unfairness is ingrained in the data itself, rather than the model. But no research has been conducted to measure the unfairness caused by a specific transformation made in the data preprocessing stage. In this paper, we introduced the causal method of fairness to reason about the fairness impact of data preprocessing stages in ML pipeline. We leveraged existing metrics to define the fairness measures of the stages. Then we conducted a detailed fairness evaluation of the preprocessing stages in 37 pipelines collected from three different sources. Our results show that certain data transformers are causing the model to exhibit unfairness. We identified a number of fairness patterns in several categories of data transformers. Finally, we showed how the local fairness of a preprocessing stage composes in the global fairness of the pipeline. We used the fairness composition to choose appropriate downstream transformer that mitigates unfairness in the machine learning pipeline.},
author = {Sumon Biswas and Hridesh Rajan},
journal = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
volume = {null},
pages = {null},
doi = {10.1145/3468264.3468536},
arxivid = {2106.06054},
}

@article{e4d1e180e63a4bfa7a7c60f0dfc0812a883d6208,
title = {FAWOS: Fairness-Aware Oversampling Algorithm Based on Distributions of Sensitive Attributes},
year = {2021},
url = {https://www.semanticscholar.org/paper/e4d1e180e63a4bfa7a7c60f0dfc0812a883d6208},
abstract = {With the increased use of machine learning algorithms to make decisions which impact people’s lives, it is of extreme importance to ensure that predictions do not prejudice subgroups of the population with respect to sensitive attributes such as race or gender. Discrimination occurs when the probability of a positive outcome changes across privileged and unprivileged groups defined by the sensitive attributes. It has been shown that this bias can be originated from imbalanced data contexts where one of the classes contains a much smaller number of instances than the other classes. It is also important to identify the nature of the imbalanced data, including the characteristics of the minority classes’ distribution. This paper presents FAWOS: a Fairness-Aware oversampling algorithm which aims to attenuate unfair treatment by handling sensitive attributes’ imbalance. We categorize different types of datapoints according to their local neighbourhood with respect to the sensitive attributes, identifying which are more difficult to learn by the classifiers. In order to balance the dataset, FAWOS oversamples the training data by creating new synthetic datapoints using the different types of datapoints identified. We test the impact of FAWOS on different learning classifiers and analyze which can better handle sensitive attribute imbalance. Empirically, we observe that this algorithm can effectively increase the fairness results of the classifiers while not neglecting the classification performance. Source code can be found at: https://github.com/teresalazar13/FAWOS},
author = {Teresa Salazar and M. S. Santos and Helder Araújo and P. Abreu},
journal = {IEEE Access},
volume = {9},
pages = {81370-81379},
doi = {10.1109/ACCESS.2021.3084121},
}

@article{8a3af84f5e78d2582f476cd220a0010595c0c737,
title = {Fairness Improvement with Multiple Protected Attributes: How Far Are We?},
year = {2023},
url = {https://www.semanticscholar.org/paper/8a3af84f5e78d2582f476cd220a0010595c0c737},
abstract = {Existing research mostly improves the fairness of Machine Learning (ML) software regarding a single protected attribute at a time, but this is unrealistic given that many users have multiple protected attributes. This paper conducts an extensive study of fairness improvement regarding multiple protected attributes, covering 11 state-of-the-art fairness improvement methods. We analyze the effectiveness of these methods with different datasets, metrics, and ML models when considering multiple protected attributes. The results reveal that improving fairness for a single protected attribute can largely decrease fairness regarding unconsidered protected attributes. This decrease is observed in up to 88.3% of scenarios (57.5% on average). More surprisingly, we find little difference in accuracy loss when considering single and multiple protected attributes, indicating that accuracy can be maintained in the multiple-attribute paradigm. However, the effect on F1-score when handling two protected attributes is about twice that of a single attribute. This has important implications for future fairness research: reporting only accuracy as the ML performance metric, which is currently common in the literature, is inadequate.},
author = {Zhenpeng Chen and Jie M. Zhang and Federica Sarro and Mark Harman},
journal = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
volume = {null},
pages = {null},
doi = {10.1145/3597503.3639083},
arxivid = {2308.01923},
}

@article{2399382f0e0fe4e50a10786cd6b304074906d1ee,
title = {Group Fairness by Probabilistic Modeling with Latent Fair Decisions},
year = {2020},
url = {https://www.semanticscholar.org/paper/2399382f0e0fe4e50a10786cd6b304074906d1ee},
abstract = {Machine learning systems are increasingly being used to make impactful decisions such as loan applications and criminal justice risk assessments, and as such, ensuring fairness of these systems is critical. This is often challenging as the labels in the data are biased. This paper studies learning fair probability distributions from biased data by explicitly modeling a latent variable that represents a hidden, unbiased label. In particular, we aim to achieve demographic parity by enforcing certain independencies in the learned model. We also show that group fairness guarantees are meaningful only if the distribution used to provide those guarantees indeed captures the real-world data. In order to closely model the data distribution, we employ probabilistic circuits, an expressive and tractable probabilistic model, and propose an algorithm to learn them from incomplete data. We show on real-world datasets that our approach not only is a better model of how the data was generated than existing methods but also achieves competitive accuracy. Moreover, we also evaluate our approach on a synthetic dataset in which observed labels indeed come from fair labels but with added bias, and demonstrate that the fair labels are successfully retrieved.},
author = {YooJung Choi and Meihua Dang and Guy Van den Broeck},
doi = {10.1609/aaai.v35i13.17431},
arxivid = {2009.09031},
}

@article{fa1103b082ba834a8dd15c7bc1d14774e5b8c36a,
title = {Fairway: a way to build fair ML software},
year = {2020},
url = {https://www.semanticscholar.org/paper/fa1103b082ba834a8dd15c7bc1d14774e5b8c36a},
abstract = {Machine learning software is increasingly being used to make decisions that affect people's lives. But sometimes, the core part of this software (the learned model), behaves in a biased manner that gives undue advantages to a specific group of people (where those groups are determined by sex, race, etc.). This "algorithmic discrimination" in the AI software systems has become a matter of serious concern in the machine learning and software engineering community. There have been works done to find "algorithmic bias" or "ethical bias" in the software system. Once the bias is detected in the AI software system, the mitigation of bias is extremely important. In this work, we a)explain how ground-truth bias in training data affects machine learning model fairness and how to find that bias in AI software,b)propose a method Fairway which combines pre-processing and in-processing approach to remove ethical bias from training data and trained model. Our results show that we can find bias and mitigate bias in a learned model, without much damaging the predictive performance of that model. We propose that (1) testing for bias and (2) bias mitigation should be a routine part of the machine learning software development life cycle. Fairway offers much support for these two purposes.},
author = {Joymallya Chakraborty and Suvodeep Majumder and Zhe Yu and T. Menzies},
journal = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
volume = {null},
pages = {null},
doi = {10.1145/3368089.3409697},
}

@article{b878fbccaa85aa9f3578bcd3c640e2e394aabcd6,
title = {An exploration of algorithmic discrimination in data and classification},
year = {2018},
url = {https://www.semanticscholar.org/paper/b878fbccaa85aa9f3578bcd3c640e2e394aabcd6},
abstract = {Algorithmic discrimination is an important aspect when data is used for predictive purposes. This paper analyzes the relationships between discrimination and classification, data set partitioning, and decision models, as well as correlation. The paper uses real world data sets to demonstrate the existence of discrimination and the independence between the discrimination of data sets and the discrimination of classification models.},
author = {Jixue Liu and Jiuyong Li and Feiyue Ye and Lin Liu and T. Le and Ping Xiong},
journal = {ArXiv},
volume = {abs/1811.02994},
pages = {null},
arxivid = {1811.02994},
}

@article{22bdaf8332c6680317b7e33eca8f4f937dae123a,
title = {Fairea: a model behaviour mutation approach to benchmarking bias mitigation methods},
year = {2021},
url = {https://www.semanticscholar.org/paper/22bdaf8332c6680317b7e33eca8f4f937dae123a},
abstract = {The increasingly wide uptake of Machine Learning (ML) has raised the significance of the problem of tackling bias (i.e., unfairness), making it a primary software engineering concern. In this paper, we introduce Fairea, a model behaviour mutation approach to benchmarking ML bias mitigation methods. We also report on a large-scale empirical study to test the effectiveness of 12 widely-studied bias mitigation methods. Our results reveal that, surprisingly, bias mitigation methods have a poor effectiveness in 49% of the cases. In particular, 15% of the mitigation cases have worse fairness-accuracy trade-offs than the baseline established by Fairea; 34% of the cases have a decrease in accuracy and an increase in bias. Fairea has been made publicly available for software engineers and researchers to evaluate their bias mitigation methods.},
author = {Max Hort and J Zhang and Federica Sarro and M. Harman},
journal = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
volume = {null},
pages = {null},
doi = {10.1145/3468264.3468565},
}

@article{df9cd156bb29106b2d985dc8d416ecde6aac4cea,
title = {Search-based Automatic Repair for Fairness and Accuracy in Decision-making Software},
year = {2024},
url = {https://www.semanticscholar.org/paper/df9cd156bb29106b2d985dc8d416ecde6aac4cea},
abstract = {S2 TL;DR: This is the first bias mitigation approach based on multi-objective search that aims to repair fairness issues without trading accuracy for binary classification methods, and it is shown that the approach successfully increases both accuracy and fairness for 61% of the cases studied.},
author = {Max Hort and Jie M. Zhang and Federica Sarro and Mark Harman},
journal = {Empirical Software Engineering},
volume = {29},
pages = {null},
doi = {10.1007/s10664-023-10419-3},
pmid = {38187986},
}

@article{a3c754aa5500e73e2455fd7c4f1bac7b708ec3ba,
title = {Do the machine learning models on a crowd sourced platform exhibit bias? an empirical study on model fairness},
year = {2020},
url = {https://www.semanticscholar.org/paper/a3c754aa5500e73e2455fd7c4f1bac7b708ec3ba},
abstract = {Machine learning models are increasingly being used in important decision-making software such as approving bank loans, recommending criminal sentencing, hiring employees, and so on. It is important to ensure the fairness of these models so that no discrimination is made based on protected attribute (e.g., race, sex, age) while decision making. Algorithms have been developed to measure unfairness and mitigate them to a certain extent. In this paper, we have focused on the empirical evaluation of fairness and mitigations on real-world machine learning models. We have created a benchmark of 40 top-rated models from Kaggle used for 5 different tasks, and then using a comprehensive set of fairness metrics, evaluated their fairness. Then, we have applied 7 mitigation techniques on these models and analyzed the fairness, mitigation results, and impacts on performance. We have found that some model optimization techniques result in inducing unfairness in the models. On the other hand, although there are some fairness control mechanisms in machine learning libraries, they are not documented. The mitigation algorithm also exhibit common patterns such as mitigation in the post-processing is often costly (in terms of performance) and mitigation in the pre-processing stage is preferred in most cases. We have also presented different trade-off choices of fairness mitigation decisions. Our study suggests future research directions to reduce the gap between theoretical fairness aware algorithms and the software engineering methods to leverage them in practice.},
author = {Sumon Biswas and Hridesh Rajan},
journal = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
volume = {null},
pages = {null},
doi = {10.1145/3368089.3409704},
arxivid = {2005.12379},
}

@article{d715f964af5dda29490f4b5a1b20f81a89b5fde7,
title = {A Methodology based on Rebalancing Techniques to Measure and Improve Fairness in Artificial Intelligence algorithms},
year = {2022},
url = {https://www.semanticscholar.org/paper/d715f964af5dda29490f4b5a1b20f81a89b5fde7},
abstract = {Artificial Intelligence (AI) has become one of the key drivers for the next decade. As important decisions are increasingly supported or directly made by AI systems, concerns regarding the rationale and fairness in their outputs are becoming more and more prominent nowadays. Following the recent interest in fairer predictions, several metrics for measuring fairness have been proposed, leading to different objectives which may need to be addressed in different fashion. In this paper, we propose (i) a methodology for analyzing and improving fairness in AI predictions by selecting sensitive attributes that should be protected; (ii) We analyze how the most common rebalance approaches affect the fairness of AI predictions and how they compare to the alternatives of removing or creating separate classifiers for each group within a protected attribute. Finally, (iii) our methodology generates a set of tables that can be easily computed for choosing the best alternative in each particular case. The main advantage of our methodology is that it allows AI practitioners to measure and improve fairness in AI algorithms in a systematic way. In order to check our proposal, we have properly applied it to the COMPAS dataset, which has been widely demonstrated to be biased by several previous studies.},
author = {Ana Lavalle and A. Maté and J. Trujillo and Jorge Garcia},
}

@article{2b8c55b0d2eb69ef948227d7baf7316c1b4159d5,
title = {Fairer Machine Learning Software on Multiple Sensitive Attributes With Data Preprocessing},
year = {2022},
url = {https://www.semanticscholar.org/paper/2b8c55b0d2eb69ef948227d7baf7316c1b4159d5},
abstract = {—This research seeks to beneﬁt the software engineering society by providing a simple yet effective approach to improve fairness of machine learning software on data with multiple sensitive attributes. Machine learning fairness has attracted increasing attention since machine learning software is increasingly used for high-stakes and high-risk decisions. Amongst all the fairness notations, this work speciﬁcally targets “equalized odds”. Equalized odds requires that members of every demographic group do not receive disparate mistreatment. It is one of the most widely accepted fairness notations given its advantage in always allowing perfect classiﬁers. Most existing solutions for machine learning fairness do not directly target equalized odds and only affect one sensitive attribute (e.g. sex) at a time. To overcome this shortage, we analyzed the condition of equalized odds and hypothesize that balancing the class distribution of training data across every demographic group will improve equalized odds of the learned model. On four real-world datasets (two of which have multiple sensitive attributes) and three synthetic datasets, our empirical results show that, at low computational overhead, the proposed preprocessing algorithm FairBalance can signiﬁcantly improve equalized odds without much, if any damage to the prediction performance. FairBalance also outperforms existing state-of-the-art approaches in terms of equalized odds. To facilitate reuse, reproduction, and validation of this work, our scripts and data are available at https://github.com/hil-se/FairBalance under an open-source Apache license (v2.0).},
author = {Member Ieee Zhe Yu and Joymallya Chakraborty and Fellow Ieee Tim Menzies},
}

@article{c4397b3f11de00e7f417e4d3fe9dfed0ec8e50ed,
title = {FairMod - Making Predictive Models Discrimination Aware},
year = {2018},
url = {https://www.semanticscholar.org/paper/c4397b3f11de00e7f417e4d3fe9dfed0ec8e50ed},
abstract = {Predictive models such as decision trees and neural networks may produce discrimination in their predictions. This paper proposes a method to post-process the predictions of a predictive model to make the processed predictions non-discriminatory. The method considers multiple protected variables together. Multiple protected variables make the problem more challenging than a simple protected variable. The method uses a well-cited discrimination metric and adapts it to allow the specification of explanatory variables, such as position, profession, education, that describe the contexts of the applications. It models the post-processing of predictions problem as a nonlinear optimization problem to find best adjustments to the predictions so that the discrimination constraints of all protected variables are all met at the same time. The proposed method is independent of classification methods. It can handle the cases that existing methods cannot handle: satisfying multiple protected attributes at the same time, allowing multiple explanatory attributes, and being independent of classification model types. An evaluation using four real world data sets shows that the proposed method is as effectively as existing methods, in addition to its extra power.},
author = {Jixue Liu and Jiuyong Li and Lin Liu and T. Le and Feiyue Ye and Gefei Li},
journal = {ArXiv},
volume = {abs/1811.01480},
pages = {null},
arxivid = {1811.01480},
}

@article{ac3e807dc123fdbe6ae1b258c4849bed0d647b6b,
title = {A Maximal Correlation Framework for Fair Machine Learning},
year = {2022},
url = {https://www.semanticscholar.org/paper/ac3e807dc123fdbe6ae1b258c4849bed0d647b6b},
abstract = {As machine learning algorithms grow in popularity and diversify to many industries, ethical and legal concerns regarding their fairness have become increasingly relevant. We explore the problem of algorithmic fairness, taking an information–theoretic view. The maximal correlation framework is introduced for expressing fairness constraints and is shown to be capable of being used to derive regularizers that enforce independence and separation-based fairness criteria, which admit optimization algorithms for both discrete and continuous variables that are more computationally efficient than existing algorithms. We show that these algorithms provide smooth performance–fairness tradeoff curves and perform competitively with state-of-the-art methods on both discrete datasets (COMPAS, Adult) and continuous datasets (Communities and Crimes).},
author = {Joshua K. Lee and Yuheng Bu and P. Sattigeri and Rameswar Panda and G. Wornell and Leonid Karlinsky and R. Feris},
journal = {Entropy},
volume = {24},
pages = {null},
doi = {10.3390/e24040461},
pmid = {35455124},
}
